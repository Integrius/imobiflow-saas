import OpenAI from 'openai'

/**
 * Servi√ßo para integra√ß√£o com OpenAI (fallback)
 *
 * Este servi√ßo serve como backup caso o Claude AI esteja indispon√≠vel.
 * Usa o modelo GPT-4o-mini para manter custos baixos.
 */
export class OpenAIService {
  private client: OpenAI | null = null
  private requestCount = 0
  private dailyCost = 0
  private model = 'gpt-4o-mini' // Mais barato

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY

    if (!apiKey) {
      console.warn('‚ö†Ô∏è  OPENAI_API_KEY n√£o configurada - fallback desabilitado')
      return
    }

    this.client = new OpenAI({
      apiKey: apiKey,
    })

    console.log('‚úÖ OpenAI Service (fallback) inicializado')
  }

  /**
   * Verifica se o servi√ßo est√° dispon√≠vel
   */
  isAvailable(): boolean {
    return !!this.client
  }

  /**
   * Gera resposta usando GPT
   *
   * @param prompt Prompt para a IA
   * @param context Contexto adicional
   * @param options Op√ß√µes de gera√ß√£o
   * @returns Resposta gerada
   */
  async generateResponse(
    prompt: string,
    context?: string,
    options?: {
      maxTokens?: number
      temperature?: number
    }
  ): Promise<string> {
    if (!this.isAvailable()) {
      throw new Error('OpenAI n√£o est√° dispon√≠vel. Configure OPENAI_API_KEY.')
    }

    try {
      const maxTokens = options?.maxTokens || 1024
      const temperature = options?.temperature || 0.7

      const completion = await this.client!.chat.completions.create({
        model: this.model,
        messages: [
          {
            role: 'user',
            content: context ? `${context}\n\n${prompt}` : prompt
          }
        ],
        max_tokens: maxTokens,
        temperature: temperature,
      })

      // Tracking de uso
      this.requestCount++
      const inputTokens = completion.usage?.prompt_tokens || 0
      const outputTokens = completion.usage?.completion_tokens || 0
      this.dailyCost += this.calculateCost(inputTokens, outputTokens)

      console.log(`üìä OpenAI Request #${this.requestCount}`, {
        inputTokens,
        outputTokens,
        dailyCost: `$${this.dailyCost.toFixed(4)}`
      })

      return completion.choices[0]?.message?.content || ''

    } catch (error: any) {
      console.error('‚ùå Erro ao chamar OpenAI:', error.message)

      // Rate limiting
      if (error.status === 429) {
        console.log('‚è≥ Rate limit atingido, aguardando 60s...')
        await this.sleep(60000)
        return this.generateResponse(prompt, context, options)
      }

      throw error
    }
  }

  /**
   * Analisa e retorna JSON estruturado
   */
  async analyze(prompt: string): Promise<any> {
    const response = await this.generateResponse(prompt)

    try {
      // Remove markdown code blocks
      const cleaned = response.replace(/```json\n?|\n?```/g, '').trim()
      return JSON.parse(cleaned)
    } catch {
      return { response }
    }
  }

  /**
   * Calcula custo baseado em tokens
   */
  private calculateCost(inputTokens: number, outputTokens: number): number {
    // Pre√ßos GPT-4o-mini (por milh√£o de tokens)
    // Fonte: https://openai.com/pricing (Dec 2024)
    const INPUT_COST_PER_M = 0.15  // $0.15 / 1M tokens
    const OUTPUT_COST_PER_M = 0.60 // $0.60 / 1M tokens

    const inputCost = (inputTokens / 1000000) * INPUT_COST_PER_M
    const outputCost = (outputTokens / 1000000) * OUTPUT_COST_PER_M

    return inputCost + outputCost
  }

  /**
   * Sleep helper
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms))
  }

  /**
   * Retorna estat√≠sticas de uso
   */
  getStats() {
    return {
      requestCount: this.requestCount,
      dailyCost: this.dailyCost,
      model: this.model
    }
  }

  /**
   * Reseta estat√≠sticas di√°rias
   */
  resetDailyStats() {
    this.requestCount = 0
    this.dailyCost = 0
  }
}

// Exporta inst√¢ncia singleton
export const openaiService = new OpenAIService()
